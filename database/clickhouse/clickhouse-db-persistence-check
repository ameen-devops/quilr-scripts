[quilradmin@ip-10-71-12-28 ~]$ kubectl get pods -n quilr | grep -i clickhouse
quilr-clickhouse-shard0-0                                    1/1     Running     0              6d1h
[quilradmin@ip-10-71-12-28 ~]$ kubectl exec -it quilr-clickhouse-shard0-0 -n quilr -- bash
I have no name!@quilr-clickhouse-shard0-0:/$ ls -lh /bitnami/clickhouse/data
total 48K
drwxr-s---.  2 1001 1001 4.0K Sep  9 13:08 access
drwxr-s---.  5 1001 1001 4.0K Sep  9 12:20 data
drwxr-s---.  2 1001 1001 4.0K Sep  9 11:12 flags
drwxr-s---.  2 1001 1001 4.0K Sep  9 11:12 format_schemas
drwxr-s---.  2 1001 1001 4.0K Sep  9 12:20 metadata
drwxr-s---.  2 1001 1001 4.0K Sep  9 11:12 metadata_dropped
drwxr-s---.  2 1001 1001 4.0K Sep  9 11:12 preprocessed_configs
-rw-r-----.  1 1001 1001   56 Sep  9 11:12 status
drwxr-s---. 15 1001 1001 4.0K Sep 14 13:10 store
drwxr-s---.  2 1001 1001 4.0K Sep  9 11:12 tmp
drwxr-s---.  2 1001 1001 4.0K Sep  9 11:12 user_files
-rw-r-----.  1 1001 1001   36 Sep  9 11:12 uuid
I have no name!@quilr-clickhouse-shard0-0:/$ cd data
bash: cd: data: No such file or directory
I have no name!@quilr-clickhouse-shard0-0:/$ ls
bin  bitnami  boot  dev  docker-entrypoint-initdb.d  docker-entrypoint-startdb.d  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
I have no name!@quilr-clickhouse-shard0-0:/$ cd bitnami/
I have no name!@quilr-clickhouse-shard0-0:/bitnami$ ls
clickhouse
I have no name!@quilr-clickhouse-shard0-0:/bitnami$ cd clickhouse/
I have no name!@quilr-clickhouse-shard0-0:/bitnami/clickhouse$ ls
data  etc  lost+found
I have no name!@quilr-clickhouse-shard0-0:/bitnami/clickhouse$ cd dtaa
bash: cd: dtaa: No such file or directory
I have no name!@quilr-clickhouse-shard0-0:/bitnami/clickhouse$ ls
data  etc  lost+found
I have no name!@quilr-clickhouse-shard0-0:/bitnami/clickhouse$ cd data
I have no name!@quilr-clickhouse-shard0-0:/bitnami/clickhouse/data$ ls
access  data  flags  format_schemas  metadata  metadata_dropped  preprocessed_configs  status  store  tmp  user_files  uuid
I have no name!@quilr-clickhouse-shard0-0:/bitnami/clickhouse/data$ exit
exit
[quilradmin@ip-10-71-12-28 ~]$ kubectl get pv pvc-fd6e32d0-5421-4472-a4eb-239ccc797de0 -n quilr -o yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/migrated-to: ebs.csi.aws.com
    pv.kubernetes.io/provisioned-by: kubernetes.io/aws-ebs
    volume.kubernetes.io/provisioner-deletion-secret-name: ""
    volume.kubernetes.io/provisioner-deletion-secret-namespace: ""
  creationTimestamp: "2025-09-09T11:12:09Z"
  finalizers:
  - external-provisioner.volume.kubernetes.io/finalizer
  - kubernetes.io/pv-protection
  - external-attacher/ebs-csi-aws-com
  labels:
    topology.kubernetes.io/region: us-east-1
    topology.kubernetes.io/zone: us-east-1a
  name: pvc-fd6e32d0-5421-4472-a4eb-239ccc797de0
  resourceVersion: "85081860"
  uid: 8cfeca9c-33ea-400b-bd47-31b967743f94
spec:
  accessModes:
  - ReadWriteOnce
  awsElasticBlockStore:
    fsType: ext4
    volumeID: vol-08de4693ee495b0ac
  capacity:
    storage: 8Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: data-quilr-clickhouse-shard0-0
    namespace: quilr
    resourceVersion: "85081832"
    uid: fd6e32d0-5421-4472-a4eb-239ccc797de0
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - us-east-1a
        - key: topology.kubernetes.io/region
          operator: In
          values:
          - us-east-1
  persistentVolumeReclaimPolicy: Delete
  storageClassName: gp2
  volumeMode: Filesystem
status:
  lastPhaseTransitionTime: "2025-09-09T11:12:09Z"
  phase: Bound
[quilradmin@ip-10-71-12-28 ~]$ 

---------------------------------------------------

Perfect â€” this YAML gives us the full picture ğŸ”.

1. Where your data is

From the PV definition:

awsElasticBlockStore:
  fsType: ext4
  volumeID: vol-08de4693ee495b0ac


Your ClickHouse data is stored on the EBS volume vol-08de4693ee495b0ac in us-east-1a.
The filesystem is ext4, and Kubernetes mounts it into the pod at /bitnami/clickhouse.

2. How to access the data if the pod isnâ€™t running

Since this is a raw EBS volume, you have two main options:

ğŸ”¹ Option A: Mount it on the Kubernetes node

Find which worker node had the pod:

kubectl get pod quilr-clickhouse-shard0-0 -n quilr -o wide


â†’ Look at the NODE column.

SSH into that node (if you have access).
The EBS volume will show up as a device (e.g. /dev/nvme1n1).

Mount it:

lsblk
sudo mkdir /mnt/clickhouse
sudo mount /dev/nvme1n1 /mnt/clickhouse
ls /mnt/clickhouse


Youâ€™ll see directories like data/ and metadata/ for ClickHouse.

âš ï¸ Caution: Donâ€™t write to this volume outside of Kubernetes if you still want to reuse it with ClickHouse. Treat it as read-only.

ğŸ”¹ Option B: Mount it on a helper EC2 instance

If the pod is stuck and you want to investigate:

Scale down the statefulset to release the PVC:

kubectl scale statefulset quilr-clickhouse-shard0 --replicas=0 -n quilr


Go to AWS Console â†’ EC2 â†’ Volumes, find vol-08de4693ee495b0ac.
Detach it from the EKS node if itâ€™s still attached.

Attach it to a helper EC2 instance in the same AZ (us-east-1a).

On that helper instance:

lsblk
sudo mkdir /mnt/clickhouse
sudo mount /dev/nvme1n1 /mnt/clickhouse
ls -lh /mnt/clickhouse


Now youâ€™ll see the same ClickHouse files.

3. What youâ€™ll actually see

The volume contains the Bitnami ClickHouse data dir (/bitnami/clickhouse).

Inside:

data/ â†’ actual tables and parts (not human-readable rows).

metadata/ â†’ table definitions.

access/ â†’ user configs.

Example layout:

/bitnami/clickhouse/
  â”œâ”€â”€ access/
  â”œâ”€â”€ data/
  â”‚   â”œâ”€â”€ default/
  â”‚   â”‚   â””â”€â”€ test_persist/
  â”‚   â”‚       â”œâ”€â”€ detached/
  â”‚   â”‚       â”œâ”€â”€ format_version.txt
  â”‚   â”‚       â”œâ”€â”€ metadata.xml
  â”‚   â”‚       â”œâ”€â”€ store/...
  â”œâ”€â”€ metadata/
  â””â”€â”€ tmp/

4. Persistence gotcha âš ï¸

Your persistentVolumeReclaimPolicy is Delete.
ğŸ‘‰ That means: if you ever delete the PVC (data-quilr-clickhouse-shard0-0), Kubernetes will delete vol-08de4693ee495b0ac in AWS.
If you want to keep the volume safe even after PVC deletion, change it to Retain:

kubectl patch pv pvc-fd6e32d0-5421-4472-a4eb-239ccc797de0 \
  -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'


âœ… Summary:
Your ClickHouse data is in EBS volume vol-08de4693ee495b0ac (ext4, us-east-1a).
If the pod isnâ€™t running, you can still mount that volume either on the Kubernetes node or on any helper EC2 instance.
Be aware of the Delete reclaim policy â€” if the PVC is deleted, the EBS volume goes too.

Do you want me to show you step-by-step how to safely make a copy/snapshot of that EBS volume so you always have a backup, even if the PVC gets deleted?
